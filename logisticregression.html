<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>The Nordic Grid - Energy Articles</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="shortcut icon" href="images/favicon.ico">
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index" class="logo"><strong>The Nordic Grid</strong> Energy Editorial</a>

								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Logistic regression for electricity distribution companies</h1>
										<blockquote>For an electricity distribution company, telecom company, social media company, etc., it is of a high interest to hold onto costumers as these companies are based on a subscription service. Keeping costumers is typically more profitable than acquire new ones, thus focusing on how to avoid costumer churn is very important for an electricity distribution company.</blockquote>
									</header>
									<!-- Content -->
										<h2 id="content">Logistic Regression</h2>
										<p>Avoiding costumer churn can be done with data analysis with Python packages. With logistic regression one can estimate the probability of whether a costumer will leave or not. The logistic model is a statistical model that uses a logistic function to model a binary dependent variable. Logistic regression estimates the parameters of a logistic model and is a form of binomial regression. A binary logistic model has a dependant variable with two possible values. Examples of values could be win/lose, healthy/sick, yes/no, churn/no churn, and these values are represented by an indicator variable where the two values are labelled 0 and 1. </p>
											
									<!-- <hr class="major" /> -->

									<h4>Telecom example</h4>
									<p>In this article an example of a telecom company will be used, but the principals are the same for an electricity distribution company. The dataset used in this article provides information to help predict what behavior will help retain customers.<br />
									The dataset includes information about:<br />
									<ul>
  <li>Customers who left within the last month – the column is called Churn</li>
  <li>Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies</li>
  <li>Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges</li>
  <li>Demographic info about customers – gender, age range, and if they have partners and dependents</li>
</ul></p>
									<h4>Getting Started</h4>
									<p>First we need to import the necessary packages:</p>
									<pre><code>import pandas as pd
import pylab as pl
import numpy as np
import scipy.optimize as opt
from sklearn import preprocessing
%matplotlib inline 
import matplotlib.pyplot as plt
</code></pre>
									<p>After importing the data to Python we compress it and convert it to integer. Compressing is done to make the data simpler to work with by selecting presumably important measures, consequently some important information is lost that could have improved the logistic regression. Remember that 1 means churn and 0 means no churn.</p>
									<p><span class="image"><img src="images/logregcompressing.jpg" alt="" /></span></p>
									<h4>Normalizing The Dataset</h4>
									<p>We define X and y for our dataset and normalize the dataset:</p>
									<pre><code>X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])
X[0:5]

y = np.asarray(churn_df['churn'])
y [0:5]
</code></pre>
									<p><span class="image"><img src="images/logregnormalize.jpg" alt="" /></span></p>
									<h4>Training And Testing</h4>
									<p>We split our data into train and test set with the following code:</p>
									<p><pre><code>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)
</code></pre></p>
									<p>This sets the data in the following manner:<br />
									<b>Train set</b>: (160, 7) (160,)<br />
									<b>Test set</b>: (40, 7) (40,)</p>
									<h4>Modeling</h4>
									<p>We build our model using LogisticRegression from Scikit-learn package. With this function logistic regression is implemented and we can used different numerical optimizers to find parameters (e.g. ‘liblinear’, ‘saga’, ‘sag’ solvers ). Logistic Regression in Scikit-learn supports regularization. This is a technique utilized to solve overfitting in machine learning models. C parameter indicates inverse of regularization strength which must be a positive float (decimal). Let us fit our model with the train set and we predict it using our test set:</p>
									<p><pre><code>from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)
yhat = LR.predict(X_test)
yhat
</code></pre></p>
									<p>The result is the following array:<br />[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]</p>
									<h4>Predicting Probability</h4>
									<p>Finally we can predict the probability of churn or no churn. We use the function <i>predict_proba</i> which returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 1, P(Y=1|X), and second column is the probability of class 0, P(Y=0|X):</p>
									<p><pre><code>yhat_prob = LR.predict_proba(X_test)
yhat_prob
</code></pre></p>
									<p>The results are given as an array. Below are the first 5 (out of 28) rows of the array:<br /><br />
									<b>[0.54132919, 0.45867081],<br />
									[0.60593357, 0.39406643],<br />
									[0.56277713, 0.43722287],<br />
									[0.63432489, 0.36567511],<br />
									[0.56431839, 0.43568161]</b></p>
									<h4>Evaluation</h4>
									<p>One way of looking at the accuracy of a classifier is to look at a <i>confusion matrix</i>.</p>
									<p><pre><code>from sklearn.metrics import classification_report, confusion_matrix
import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
	
	fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
print(confusion_matrix(y_test, yhat, labels=[1,0]))

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])
np.set_printoptions(precision=2)


# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')
</code></pre></p>
									<span class="image"><img src="images/logregfconfusion.jpg" alt="" /></span>
									<p>The first row is for customers whose actual churn value in test set is 1. As you can calculate, out of 40 customers, the churn value of 15 of them is 1. And out of these 15, the classifier correctly predicted 6 of them as 1, and 9 of them as 0.
									<p>It means, for 6 customers, the actual churn value were 1 in the test set, and classifier also correctly predicted those as 1. However, while the actual label of 9 customers were 1, the classifier predicted those as 0, which is not very good. We can consider it as error of the model for first row.</p>
									<p>For the customers with churn value 0 we look at the second row. It looks like there were 25 customers with churn value of 0. The classifier correctly predicted 24 of them as 0, and one of them wrongly as 1. So, it has done a good job in predicting the customers with churn value 0. A good thing about the confusion matrix is that it shows the ability of the model to correctly predict or separate the classes. In specific case of binary classifier we can interpret these numbers as the count of true positives, false positives, true negatives, and false negatives.</p>
				
									<p>Ultimately we find the F1 score. The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show if a classifer has a good value for recall and precision. The average accuracy for this classifier is 0.72 in our case.</p>
									<p><pre><code>print (classification_report(y_test, yhat))
</code></pre></p>
									<span class="image"><img src="images/logregf1.jpg" alt="" /></span>
													<!-- Box with sources-->
													<h3>Sources and credits</h3>
													<div class="box">
														<p>Jupyter Notebook used with Python inspired by IBM course material.</p>
													</div>

								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index"><b>Homepage</b></a></li>
										<li>
											<span class="opener"><b>Articles</b></span>
											<ul>
												<li><a href="energyarticles">Energy</a></li>
												<li><a href="powermarketarticles">Power Market</a></li>
												<li><a href="economyarticles">Economy</a></li>
											</ul>
										</li>
										<li><a href="programming"><b>Programming</b></a></li>
									</ul>
								</nav>


							<!-- Section -->
								<section>
									<h4>Article By</h4>
													<p><span class="image left"><img src="images/picid.png" alt="" /></span>Andreas Olav Hallingstad, Civil Engineering Student.<br>Follow him on <a href="https://www.linkedin.com/in/andreasolavhallingstad/">LinkedIn</a></p>
								</section>
								
							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; The Nordic Grid<br>Images: <a href="https://unsplash.com">Unsplash</a><br>Run by <a href="https://www.linkedin.com/in/andreasolavhallingstad/">Andreas</a><br>Design: <a href="https://html5up.net">HTML5 UP</a></p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>